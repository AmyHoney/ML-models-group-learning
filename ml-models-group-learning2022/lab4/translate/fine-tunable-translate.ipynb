{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "707ffea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export http_proxy=http://proxy.vmware.com:3128\n",
    "!export https_proxy=http://proxy.vmware.com:3128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3970b7",
   "metadata": {},
   "source": [
    "# Lab4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c52811",
   "metadata": {},
   "source": [
    "### Fine-tuning a model on a translation task\n",
    "In this notebook, we will see how to fine-tune one of the ðŸ¤— Transformers model for a translation task. We will use the WMT dataset, a machine translation dataset composed from a collection of various sources, including news commentaries and parliament proceedings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0384094",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "11e90993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['acronym_identification', 'ade_corpus_v2', 'adversarial_qa', 'aeslc', 'afrikaans_ner_corpus', 'ag_news', 'ai2_arc', 'air_dialogue', 'ajgt_twitter_ar', 'allegro_reviews']\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "dataset_lists = datasets.list_datasets()\n",
    "print(dataset_lists[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "642279dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset wmt16 (/root/.cache/huggingface/datasets/wmt16/ro-en/1.0.0/9e0038fe4cc117bd474d2774032cc133e355146ed0a47021b2040ca9db4645c0)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f0c6a4b8274d719a622a9d458e1b21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 610320\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1999\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1999\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "raw_datasets = datasets.load_dataset(\"wmt16\", \"ro-en\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fd46957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'en': 'Turkey's second most visited museum after Topkapi Palace, the Mevlana Museum in Konya, Anatolia, is undergoing its largest-ever restoration, and the first comprehensive restoration since 1926.', 'ro': 'Al doilea muzeu ca numÄƒr de vizitatori din Turcia dupÄƒ Palatul Topkapi, Muzeul Mevlana din Konya, Anatolia, trece prin cea mai amplÄƒ restaurare a sa, aceasta fiind prima sa restaurare completÄƒ din 1926.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'en': 'The tribunal wants Serbia to extradite four more indictees, including Bosnian Serb wartime commander Ratko Mladic and Bosnian Serb wartime leader Radovan Karadzic.', 'ro': 'Tribunalul doreÅŸte ca Serbia sÄƒ extrÄƒdeze Ã®ncÄƒ patru inculpaÅ£i, printre care se numÄƒrÄƒ comandantul sÃ¢rb bosniac din timpul rÄƒzboiului Ratko Mladic ÅŸi liderul sÃ¢rb bosniac Radovan Karadzic.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'en': 'At the end of the month, parties will be convened to start formal negotiations about police restructuring, Ashdown said.', 'ro': 'La sfÃ¢rÅŸitul lunii, pÄƒrÅ£ile vor fi convocate pentru lansarea negocierilor oficiale privitoare la restructurarea poliÅ£iei, a afirmat Ashdown.'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'en': '(Sustained applause)', 'ro': '(Aplauze prelungite)'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'en': 'Let us have good discussions, then, but let us also play hard ball and let us, I beg, secure the energy supplies of our Member States.', 'ro': 'HaideÈ›i aÈ™adar sÄƒ purtÄƒm discuÈ›ii fructuoase, dar haideÈ›i sÄƒ jucÄƒm dur È™i sÄƒ asigurÄƒm, vÄƒ rog, aprovizionarea cu energie a statelor membre.'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To get a sense of what the data looks like, the following function will show some examples picked randomly in the dataset.\n",
    "\n",
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "    \n",
    "show_random_elements(raw_datasets[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea19bdd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from /root/.cache/huggingface/modules/datasets_modules/metrics/sacrebleu/31e1673407d8789b8f5ddfd979948f6a1de0a6d691426d55fa74a35ffb0c1bdf (last modified on Mon Jul  4 08:03:55 2022) since it couldn't be found locally at sacrebleu, or remotely on the Hugging Face Hub.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metric(name: \"sacrebleu\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Sequence(feature=Value(dtype='string', id='sequence'), length=-1, id='references')}, usage: \"\"\"\n",
      "Produces BLEU scores along with its sufficient statistics\n",
      "from a source against one or more references.\n",
      "\n",
      "Args:\n",
      "    predictions (`list` of `str`): list of translations to score. Each translation should be tokenized into a list of tokens.\n",
      "    references (`list` of `list` of `str`): A list of lists of references. The contents of the first sub-list are the references for the first prediction, the contents of the second sub-list are for the second prediction, etc. Note that there must be the same number of references for each prediction (i.e. all sub-lists must be of the same length).\n",
      "    smooth_method (`str`): The smoothing method to use, defaults to `'exp'`. Possible values are:\n",
      "        - `'none'`: no smoothing\n",
      "        - `'floor'`: increment zero counts\n",
      "        - `'add-k'`: increment num/denom by k for n>1\n",
      "        - `'exp'`: exponential decay\n",
      "    smooth_value (`float`): The smoothing value. Only valid when `smooth_method='floor'` (in which case `smooth_value` defaults to `0.1`) or `smooth_method='add-k'` (in which case `smooth_value` defaults to `1`).\n",
      "    tokenize (`str`): Tokenization method to use for BLEU. If not provided, defaults to `'zh'` for Chinese, `'ja-mecab'` for Japanese and `'13a'` (mteval) otherwise. Possible values are:\n",
      "        - `'none'`: No tokenization.\n",
      "        - `'zh'`: Chinese tokenization.\n",
      "        - `'13a'`: mimics the `mteval-v13a` script from Moses.\n",
      "        - `'intl'`: International tokenization, mimics the `mteval-v14` script from Moses\n",
      "        - `'char'`: Language-agnostic character-level tokenization.\n",
      "        - `'ja-mecab'`: Japanese tokenization. Uses the [MeCab tokenizer](https://pypi.org/project/mecab-python3).\n",
      "    lowercase (`bool`): If `True`, lowercases the input, enabling case-insensitivity. Defaults to `False`.\n",
      "    force (`bool`): If `True`, insists that your tokenized input is actually detokenized. Defaults to `False`.\n",
      "    use_effective_order (`bool`): If `True`, stops including n-gram orders for which precision is 0. This should be `True`, if sentence-level BLEU will be computed. Defaults to `False`.\n",
      "\n",
      "Returns:\n",
      "    'score': BLEU score,\n",
      "    'counts': Counts,\n",
      "    'totals': Totals,\n",
      "    'precisions': Precisions,\n",
      "    'bp': Brevity penalty,\n",
      "    'sys_len': predictions length,\n",
      "    'ref_len': reference length,\n",
      "\n",
      "Examples:\n",
      "\n",
      "    Example 1:\n",
      "        >>> predictions = [\"hello there general kenobi\", \"foo bar foobar\"]\n",
      "        >>> references = [[\"hello there general kenobi\", \"hello there !\"], [\"foo bar foobar\", \"foo bar foobar\"]]\n",
      "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
      "        >>> results = sacrebleu.compute(predictions=predictions, references=references)\n",
      "        >>> print(list(results.keys()))\n",
      "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "        >>> print(round(results[\"score\"], 1))\n",
      "        100.0\n",
      "\n",
      "    Example 2:\n",
      "        >>> predictions = [\"hello there general kenobi\",\n",
      "        ...                 \"on our way to ankh morpork\"]\n",
      "        >>> references = [[\"hello there general kenobi\", \"hello there !\"],\n",
      "        ...                 [\"goodbye ankh morpork\", \"ankh morpork\"]]\n",
      "        >>> sacrebleu = datasets.load_metric(\"sacrebleu\")\n",
      "        >>> results = sacrebleu.compute(predictions=predictions,\n",
      "        ...                             references=references)\n",
      "        >>> print(list(results.keys()))\n",
      "        ['score', 'counts', 'totals', 'precisions', 'bp', 'sys_len', 'ref_len']\n",
      "        >>> print(round(results[\"score\"], 1))\n",
      "        39.8\n",
      "\"\"\", stored examples: 0)\n"
     ]
    }
   ],
   "source": [
    "# get the metric we need to use for evaluation\n",
    "from datasets import load_metric\n",
    "metric = load_metric(\"sacrebleu\")\n",
    "print(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9db6c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': 0.0,\n",
       " 'counts': [4, 2, 0, 0],\n",
       " 'totals': [4, 2, 0, 0],\n",
       " 'precisions': [100.0, 100.0, 0.0, 0.0],\n",
       " 'bp': 1.0,\n",
       " 'sys_len': 4,\n",
       " 'ref_len': 4}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can call its compute method with your predictions and labels, which need to be list of decoded strings (list of list for the labels):\n",
    "fake_preds = [\"hello there\", \"general kenobi\"]\n",
    "fake_labels = [[\"hello there\"], [\"general kenobi\"]]\n",
    "metric.compute(predictions=fake_preds, references=fake_labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f290b62",
   "metadata": {},
   "source": [
    "## Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "674e8cc1",
   "metadata": {},
   "source": [
    "Before we can feed those texts to our model, we need to preprocess them. This is done by a ðŸ¤— Transformers Tokenizer which will (as the name indicates) tokenize the inputs (including converting the tokens to their corresponding IDs in the pretrained vocabulary) and put it in a format the model expects, as well as generate the other inputs that model requires.\n",
    "\n",
    "To do all of this, we instantiate our tokenizer with the AutoTokenizer.from_pretrained method, which will ensure:\n",
    "\n",
    "we get a tokenizer that corresponds to the model architecture we want to use,\n",
    "we download the vocabulary used when pretraining this specific checkpoint.\n",
    "That vocabulary will be cached, so it's not downloaded again the next time we run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d693263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "428f13b0940148039eb983e9d1eccb03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/770k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb5c7539fb00426f8fc3896af8af6b6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/798k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "220461d4504a4e948bb56a8d8274c8f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.33M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/transformers/models/marian/tokenization_marian.py:198: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"Helsinki-NLP/opus-mt-en-ro\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8a64ff",
   "metadata": {},
   "source": [
    "For the mBART tokenizer (like we have here), we need to set the source and target languages (so the texts are preprocessed properly). You can check the language codes here if you are using this notebook on a different pairs of languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8d12f2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"mbart\" in model_checkpoint:\n",
    "    tokenizer.src_lang = \"en-XX\"\n",
    "    tokenizer.tgt_lang = \"ro-RO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1559450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[125, 778, 3, 63, 141, 9191, 23, 0], [187, 32, 716, 9191, 2, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can directly call this tokenizer on one sentence or a pair of sentences:\n",
    "tokenizer([\"Hello, this one sentence!\", \"This is another sentence.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea00b55d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"translate English to Romanian: \"\n",
    "else:\n",
    "    prefix = \"\"\n",
    "\n",
    "print(prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fb7cc1",
   "metadata": {},
   "source": [
    "We can then write the function that will preprocess our samples. We just feed them to the tokenizer with the argument truncation=True. This will ensure that an input longer that what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85c7eea4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'function'=<function preprocess_function at 0x7f1f50f15dc0> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a2c40beec024727937cd21a46a10b96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/611 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1d93585e9f4ac6bae480b229ce74aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ef1bf4adcd941deabf8aa4a407b1cef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"en\"\n",
    "target_lang = \"ro\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "preprocess_function(raw_datasets['train'][:2])\n",
    "tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7556d7",
   "metadata": {},
   "source": [
    "# Fine-tuning the model\n",
    "Now that our data is ready, we can download the pretrained model and fine-tune it. Since our task is of the sequence-to-sequence kind, we use the AutoModelForSeq2SeqLM class. Like with the tokenizer, the from_pretrained method will download and cache the model for us.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcc11772",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "batch_size = 16\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned-{source_lang}-to-{target_lang}\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # if cpuï¼Œset fp16=False; If gpu, set fp16=True\n",
    "    #push_to_hub=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17011dc",
   "metadata": {},
   "source": [
    "The last thing to define for our Seq2SeqTrainer is how to compute the metrics from the predictions. We need to define a function for this, which will just use the metric we loaded earlier, and we have to do a bit of pre-processing to decode the predictions into texts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b51523ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Some simple post-processing\n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "\n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v, 4) for k, v in result.items()}\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03074d0",
   "metadata": {},
   "source": [
    "Then we just need to pass all of this along with our datasets to the Seq2SeqTrainer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cc046901",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "The following columns in the training set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "/opt/conda/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 610320\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 38145\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38145' max='38145' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38145/38145 1:07:23, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.743900</td>\n",
       "      <td>1.289592</td>\n",
       "      <td>28.014800</td>\n",
       "      <td>34.109100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-4500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-12500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-13500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-14500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-15500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-16500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-17500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-18500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-19500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-20500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-21500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-22500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-23500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-24500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-25500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-26500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-27500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-28500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-29500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-30500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-31500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-32500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-33500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-34500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-35500] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37500\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37500/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36000] due to args.save_total_limit\n",
      "Saving model checkpoint to opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000\n",
      "Configuration saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000/config.json\n",
      "Model weights saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in opus-mt-en-ro-finetuned-en-to-ro/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [opus-mt-en-ro-finetuned-en-to-ro/checkpoint-36500] due to args.save_total_limit\n",
      "The following columns in the evaluation set don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation. If translation are not expected by `MarianMTModel.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1999\n",
      "  Batch size = 16\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=38145, training_loss=0.7716991539066468, metrics={'train_runtime': 4043.8441, 'train_samples_per_second': 150.926, 'train_steps_per_second': 9.433, 'total_flos': 1.0093708497125376e+16, 'train_loss': 0.7716991539066468, 'epoch': 1.0})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c4123b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
